{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}\n",
      "Number of partitions: 10\n",
      "partition #0 length: 300\n",
      "partition #1 length: 323\n",
      "partition #2 length: 366\n",
      "partition #3 length: 491\n",
      "partition #4 length: 491\n",
      "partition #5 length: 491\n",
      "partition #6 length: 373\n",
      "partition #7 length: 673\n",
      "partition #8 length: 440\n",
      "partition #9 length: 524\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, ArrayType\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql.functions import row_number, lit\n",
    "from pyspark.sql.window import Window\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Read two spatio-textual files\n",
    "df1 = spark.read.csv(\"C:/Users/nikos/Desktop/partitiondata.csv\", header=False).toDF(\"userId\", \"Text\", \"Latitude\", \"Longitude\")\n",
    "df2 = spark.read.csv(\"C:/Users/nikos/Desktop/check-ins.csv\", header=False).toDF(\"userId\", \"Text\", \"Latitude\", \"Longitude\")\n",
    "\n",
    "gps = df1.withColumn(\"Latitude\", regexp_replace('Latitude', '[\"(]', '').cast(\"float\")).withColumn(\"Longitude\", regexp_replace('Longitude', '[)\"]', '').cast(\"float\")).withColumn('DatasetID', lit(\"A\"))\n",
    "\n",
    "check = df2.withColumn(\"Latitude\", regexp_replace('Latitude', '[\"(]', '').cast(\"float\")).withColumn(\"Longitude\", regexp_replace('Longitude', '[)\"]', '').cast(\"float\")).withColumn('DatasetID', lit(\"B\"))\n",
    "\n",
    "#Union the dataframes and sort by Latitude\n",
    "datasets = gps.union(check)\n",
    "sorted_by_latitude = datasets.sort(col(\"Latitude\").asc())\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#Function of lemmatization the text\n",
    "def lemmatization(x):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    #stemmer = SnowballStemmer(\"english\")\n",
    "    lista = []\n",
    "    words = x.split(',')\n",
    "    #words = nltk.word_tokenize(x)\n",
    "    remove = [item for item in words if item not in stop]\n",
    "    \n",
    "    for word in remove:\n",
    "        lista.append(wordnet_lemmatizer.lemmatize(word))   \n",
    "     \n",
    "    return lista\n",
    "\n",
    "udf_lemmatization = udf(lemmatization, ArrayType(StringType()))\n",
    "\n",
    "lemma_df = sorted_by_latitude.withColumn(\"Lemma\", udf_lemmatization(sorted_by_latitude.Text)).drop('Text')\n",
    "\n",
    "#γενικά χρειάζομαι το νούμερο της γραμμής για να κάνω το οριζόντιο partitioning με τη συνάρτηση cellID\n",
    "#διότι με τη διαίρεση του μεγέθους του dataframe με το 10, και παίρνοντας το ακέραιο μέρος της διαίρεσης του με το νούμερο της γραμμής, αντιστοιχίζεται στο αντίστοιχο κελί.\n",
    "#επομένως ο μόνος τρόπος για να βρίσκω το νούμερο της γραμμής είναι οι δύο επόμενες εντολές, οι οποίες δε ξέρω αν είναι αποδοτικές\n",
    "#είχα βρει και το monotonically_increasing_id αλλά δε μου βγάζει πάντα με τη σειρά του αριθμούς αλλά διακριτούς αριθμούς που δε μου κάνουν\n",
    "#αν έχεις κάποια άλλη πρόταση για να βρίσκω πιο αποδοτικά το νούμερο της γραμμής ενημέρωσε με\n",
    "w = Window().orderBy(lit('A'))\n",
    "\n",
    "dataframe = lemma_df.withColumn(\"IncreasingId\", (row_number().over(w)-1))\n",
    "\n",
    "dimension = 10\n",
    "size = dataframe.count()/dimension\n",
    "\n",
    "def cellID(x):\n",
    "\n",
    "    cellid = int(x / size) \n",
    "    return cellid\n",
    "\n",
    "udf_cellID = udf(cellID, IntegerType())\n",
    "\n",
    "cell_point = dataframe.withColumn(\"cellid\", udf_cellID(dataframe.IncreasingId))\n",
    "          \n",
    "y_axis = []\n",
    "\n",
    "#Find the maximum latitude value of each cell for the duplication of points\n",
    "for i in range(0,(dimension-1)):\n",
    "    value = cell_point.where(cell_point.cellid == i).select(max('Latitude')).collect()[0]\n",
    "    y_axis.append(value)\n",
    "\n",
    "  \n",
    "#radius of search\n",
    "r = 0.5\n",
    "\n",
    "#Function that duplicates the points of dataset B to the nearest cells based on radius\n",
    "def find_cell(y, cell, dataset):\n",
    "    l = []\n",
    "    l.append(cell)\n",
    "    if dataset == \"B\":\n",
    "        #check the lower cells to duplicate point\n",
    "        for i in range((cell - 1), -1):\n",
    "            if (i >= 0) and ((y - r) < y_axis[i][0]):\n",
    "                l.append(i)\n",
    "            else:\n",
    "                break\n",
    "        #check the upper cells to duplicate point\n",
    "        for j in range((cell + 1), dimension):\n",
    "            if (cell <= dimension) and ((y + r) > y_axis[j - 1][0]):\n",
    "                l.append(j)\n",
    "            else:\n",
    "                break\n",
    "    return l\n",
    "\n",
    "udf_cell = udf(find_cell, ArrayType(IntegerType()))\n",
    "\n",
    "duplicate_points = cell_point.withColumn(\"cell\", udf_cell(cell_point.Latitude, cell_point.cellid, cell_point.DatasetID))\n",
    "\n",
    "#make the duplication of the row\n",
    "duplicated = duplicate_points.withColumn('grid', explode(duplicate_points.cell)).drop('cell').drop('cellid')\n",
    "\n",
    "#find the distinct values to partition data\n",
    "mapping = {k: i for i, k in enumerate(\n",
    "    duplicated.select(\"grid\").distinct().orderBy(asc(\"grid\")).rdd.flatMap(lambda x: x).collect()\n",
    ")}\n",
    "                        \n",
    "print(mapping)\n",
    "\n",
    "#partition by the distinct cell id\n",
    "result = (duplicated\n",
    "    .select(\"grid\", struct([c for c in duplicated.columns]))\n",
    "    .rdd.partitionBy(len(mapping), lambda k: mapping[k])\n",
    "    .values()\n",
    "    .toDF(duplicated.schema))\n",
    "\n",
    "print(\"Number of partitions: {}\".format(result.rdd.getNumPartitions()))\n",
    "\n",
    "partitions = result.rdd.glom().collect()\n",
    "for i, l in enumerate(partitions): \n",
    "    print (\"partition #{} length: {}\".format(i, len(l)))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find points of dataset \"A\" and \"B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points of A, B in each partition: [[277, 23], [257, 66], [175, 191], [300, 191], [300, 191], [227, 264], [0, 373], [0, 673], [216, 757], [248, 602]]\n"
     ]
    }
   ],
   "source": [
    "#Find points of dataset A and B in each partition\n",
    "def function1(iterator):\n",
    "    \n",
    "    #retain the values of each partition to a pandas dataframe\n",
    "    d = pd.DataFrame([p[0], p[1], p[2], p[3], p[4], p[5], p[6]] for p in iterator)\n",
    "    d.columns = ['usedid', 'latitude', 'longitude', 'dataset', 'text', 'increasingid', 'cellid']\n",
    "    \n",
    "    a = 0\n",
    "    b = 0\n",
    "    number = []\n",
    "    \n",
    "    for row1 in d.iterrows():\n",
    "        if row1[1][3] == \"A\":\n",
    "            a += 1\n",
    "        elif row1[1][3] == \"B\":\n",
    "            b += 1\n",
    "                                    \n",
    "    number.append(a)\n",
    "    number.append(b)\n",
    "    return [number]\n",
    "\n",
    "nearest_points = result.rdd \\\n",
    "        .mapPartitions(function1) \\\n",
    "        .collect()\n",
    "    \n",
    "print(\"points of A, B in each partition: {}\".format(nearest_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions of Jaccard Similarity and Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#function which computes the jaccard similarity\n",
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection / union)\n",
    "\n",
    "#function which computes the euclidean distance\n",
    "def euclidean_distance(object1, object2):\n",
    "    x1 = object1[2]\n",
    "    x2 = object2[2]\n",
    "    y1 = object1[1]\n",
    "    y2 = object2[1]\n",
    "    dist = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "    return dist\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute-force in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time, count of using functions, count of join points: [['2.9914004802703857', 6371, 0], ['8.918791770935059', 16962, 0], ['23.199210166931152', 33425, 210], ['34.793915033340454', 57300, 0], ['34.433778285980225', 57300, 0], ['42.04338455200195', 59928, 0], ['29.76351571083069', 0, 0], ['78.77758955955505', 0, 0], ['137.3939447402954', 163512, 0], ['101.38583636283875', 149296, 0]]\n"
     ]
    }
   ],
   "source": [
    "#compute the euclidean distance and jaccard similarity for each point of dataset B against all of dataset A\n",
    "def function2(iterator):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    d = pd.DataFrame([p[0], p[1], p[2], p[3], p[4], p[5], p[6]] for p in iterator)\n",
    "    d.columns = ['usedid', 'latitude', 'longitude', 'dataset', 'text', 'increasingid', 'cellid']\n",
    "    y = []\n",
    "    \n",
    "    #textual similarity\n",
    "    e = 0.7\n",
    "    #counters of use of functions and join of points\n",
    "    count_functions= 0\n",
    "    count_join = 0\n",
    "    \n",
    "    for row1 in d.iterrows():\n",
    "        \n",
    "        x = []\n",
    "        if  (row1[1][3] == \"B\"):\n",
    "            \n",
    "            for row2 in d.iterrows():\n",
    "                if (row2[1][3] == \"A\"):\n",
    "                    count_functions += 1\n",
    "                    \n",
    "                    if (euclidean_distance(row1[1], row2[1]) <= r) and (jaccard_similarity(row1[1][4], row2[1][4]) >= e):\n",
    "                        count_join += 1\n",
    "                        x.append(row1[1][5])\n",
    "            \n",
    "        y.append(x)\n",
    "                                  \n",
    "    return [[str(time.time() - start_time), count_functions, count_join]]\n",
    "\n",
    "nearest_points = result.rdd \\\n",
    "        .mapPartitions(function2) \\\n",
    "        .collect()\n",
    "    \n",
    "print(\"Execution time, count of using functions, count of join points: {}\".format(nearest_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plane sweep in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time, count euclidean function, count jaccard function, count join: [['16.49152708053589', 6371, 6371, 0], ['22.38470482826233', 16962, 16962, 0], ['32.467488050460815', 33425, 33425, 210], ['57.770145416259766', 57300, 57300, 0], ['57.929160833358765', 57300, 57300, 0], ['44.17149353027344', 43357, 43357, 0], ['16.42401695251465', 0, 0, 0], ['46.989752531051636', 0, 0, 0], ['119.73594975471497', 150336, 417, 0], ['99.18936824798584', 136452, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "#sort values by Longitude, apply plane sweep until the longitude of a point is greater than the longitude of the checked point + radius \n",
    "def function3(iterator):\n",
    "    #retain the start time of the execution of each partition\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #retain the values of each partition to a pandas dataframe and sort them based on the longitude\n",
    "    d = pd.DataFrame([p[0], p[1], p[2], p[3], p[4], p[5], p[6]] for p in iterator)\n",
    "    d.columns = ['usedid', 'latitude', 'longitude', 'dataset', 'text', 'increasingid', 'cellid']\n",
    "    d.sort_values(by=['longitude'])\n",
    "    \n",
    "    #creates the counters of the metrics as zero\n",
    "    count_euclidean = 0\n",
    "    count_jaccard = 0\n",
    "    count_join = 0\n",
    "    \n",
    "    #εδώ θέλω να αρχικοποιήσω μια λίστα στην οποία θα κρατάω στη θέση του id της γραμμής για το dataset A το στοιχείο από το dataset B, το οποίο περνά από τις δύο συναρτήσεις\n",
    "    #επειδή όμως το id δεν αρχίζει από 0 αλλά αυξάνεται συνέχεια, δε μπορούσα να το κάνω διότι η λίστα ξεκινάει με δείκτη 0 και ανεβαίνει\n",
    "    #έτσι σκέφτηκα να φτιάχνω μια λίστα με μέγεθος βάσει της μέγιστης τιμής του id και της ελάχιστης, και πιο κάτω στη συνάρτηση να αφαιρώ το id της γραμμής από την ελάχιστη τιμή του id σε κάθε partition\n",
    "    #για παράδειγμα, αν είχα ελάχιστη τιμή id 1600 και έλεγχα το 1601, θα έβαζα στη θέση 1601-1600 = 1 της λίστας αυτό που ήθελα\n",
    "    #ξέρω ότι έτσι θα δημιουργήσω μια μεγάλη λίστα που δεν την χρειάζομαι με ακρετά κενά πεδία, αλλά ο έλεγχος που πραγματοποιώ πιο κάτ στη συνάρτηση, γίνεται μ΄΄ονο κατά + r\n",
    "    #αν έχεις κάτι άλλο να μου προτείνεις ενημέρωσε με σε παρακαλώ\n",
    "    \n",
    "    minvalue = d['increasingid'].min()\n",
    "    maxvalue = d['increasingid'].max()\n",
    "    points = [[] for x in range(minvalue, maxvalue)]\n",
    "     \n",
    "    e = 0.7\n",
    "    #check each point from its next points, until the longitude of a point is greater than the longitude of the checked point + r\n",
    "    for i, row1 in enumerate(d.iterrows()):\n",
    "        \n",
    "        for j, row2 in enumerate(d[i+1:].iterrows()):\n",
    "            \n",
    "            if row2[1][2] < (row1[1][2] + r):\n",
    "                if row1[1][3] != row2[1][3]:\n",
    "                    count_euclidean += 1\n",
    "                    \n",
    "                    if row1[1][3] == \"A\" and (euclidean_distance(row1[1], row2[1]) <= r):\n",
    "                        count_jaccard += 1\n",
    "                        if jaccard_similarity(row1[1][4], row2[1][4]) >= e:\n",
    "                            count_join += 1 \n",
    "                            points[row1[1][5]-minvalue].append(row2[1][5])\n",
    "                            \n",
    "                    elif row1[1][3] == \"B\" and (euclidean_distance(row1[1], row2[1]) <= r):\n",
    "                        count_jaccard += 1 \n",
    "                        if jaccard_similarity(row1[1][4], row2[1][4]) >= e:\n",
    "                            count_join += 1\n",
    "                            points[row2[1][5]-minvalue].append(row1[1][5])\n",
    "            else:\n",
    "                break\n",
    "                                    \n",
    "    return [[str(time.time() - start_time), count_euclidean, count_jaccard, count_join]]\n",
    "\n",
    "nearest_points = result.rdd \\\n",
    "        .mapPartitions(function3) \\\n",
    "        .collect()\n",
    "\n",
    "#partition_time = [float(i) for i in nearest_points]\n",
    "print(\"Execution time, count euclidean function, count jaccard function, count join: {}\".format(nearest_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
